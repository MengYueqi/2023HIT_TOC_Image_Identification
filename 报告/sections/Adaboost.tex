\subsection{AdaBoost简介}

AdaBoost是Adaptive+Boosting的组合词，其算法的定义如下： \par
弱分类器(weak classifier)通过顺序(sequential)学习相互补充，并将它们组合在一起以最终提高强分类器(strong classifier)的性能。 \par

工作原理如下：\par
弱分类器(weak classifier)一次一个地顺序进行学习。首先学习的分类器会产生正确分类的数据和错误分类的数据。
首先学习的分类器将正确分类的结果信息和错误分类的结果信息传递给下一分类器。
下一分类器利用从前一个分类器接收到的信息来提高分类不佳的数据的权重(weight)。
也就是说,通过不断调整前一个分类器错误分类样本的权重，使其更集中于错误分类的数据，从而使学习效果更好。
因此，名称中带有“adaptive”。
最终分类器(strong classifier)通过对先前学习的弱分类器分别应用权重并进行组合来进行学习。 \par

总结一下，就是将预测性能较低的弱分类器组合在一起，最终形成一个性能稍好一些的强分类器。
弱分类器通过相互补充(adaptive)的方式进行学习，并通过组合这些弱分类器来形成一个分类器，因此称为 boosting。 \par

用公式表示如下：
\begin{gather}
    H(x) = \alpha_1 h_1(x) + \alpha_2 h_2(x) + \dots + \alpha_t h_t(x) = \sum_{t = 1}^{T} \alpha_t h_t(x)
\end{gather}
其中：\par
$H(x)$：最终强分类器，也称为加权多数投票分类器。 \par
$h$：弱分类器，也称为基分类器。 \par
$\alpha$：弱分类器的权重，用于衡量弱分类器对最终分类器的重要性。 \par
$t$：迭代次数，表示弱分类器的数量。 \par

\noindent AdaBoost 算法的工作原理如下： \par
1. 初始化训练数据集的每个样本的权重为 1/N，其中 N 是训练数据集的样本数。\par
2. 训练一个弱分类器。\par
3. 计算弱分类器的错误分类率。\par
4. 将错误分类率高的样本的权重增加，将错误分类率低的样本的权重减少。\par
5. 重复步骤 2-4，直到满足某个终止条件。\par
6. 将所有弱分类器的输出通过加权求和得到最终的强分类器的输出。\par


\noindent AdaBoost 算法具有以下优点：\par
1. 可以有效地提高弱分类器的性能。\par
2. 可以处理异常值。\par
3. 可以处理不平衡数据集。\par
4. AdaBoost 算法在分类、回归、异常检测等领域都有广泛应用。\par